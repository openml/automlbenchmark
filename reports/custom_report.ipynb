{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "automlbenchmark_path = \"..\"\n",
    "amlb_dir = os.path.realpath(os.path.expanduser(automlbenchmark_path))\n",
    "amlb_reports_dir = os.path.join(amlb_dir, 'reports')\n",
    "for lib in [amlb_dir, amlb_reports_dir]:\n",
    "    sys.path.insert(0, lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./reports_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from report import draw_score_heatmap, draw_score_parallel_coord, draw_score_pointplot, draw_score_stripplot, draw_score_barplot\\\n",
    "                  ,prepare_results, render_leaderboard, render_metadata, render_summary\n",
    "from report.config import *\n",
    "from report.util import create_file, display\n",
    "from report.visualizations.util import register_colormap, render_colormap, savefig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare custom runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comparing results for one fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = \"1h8c\"\n",
    "results_dir = \".\"\n",
    "output_dir = \".\"\n",
    "\n",
    "included_frameworks = []\n",
    "excluded_frameworks = []\n",
    "frameworks_sort_key = None\n",
    "# frameworks_sort_key = lambda f: definitions[f]['key'] if 'key' in definitions[f] else f.lower()\n",
    "frameworks_labels = None\n",
    "# frameworks_labels = lambda l: definitions[l]['framework'].lower()\n",
    "duplicates_handling = 'fail' # accepted values: 'fail', 'keep_first', 'keep_last', 'keep_none'\n",
    "imputation = None\n",
    "normalization = None\n",
    "# normalization = (0, 'h2o', 'mean')\n",
    "row_filter = None\n",
    "# row_filter = lamdba r: r.fold == 0     #! r is a pd.Series\n",
    "title_extra = \"\"\n",
    "# register_colormap(config.colormap, ('colorblind', [1, 0, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell  is an example showing how to use/customize this notebook depending on your results\n",
    "\n",
    "results_dir = \"../results\"\n",
    "output_dir = \"./tmp\"\n",
    "duplicates_handling = 'keep_last'\n",
    "normalization = (0, 'constantpredictor', 'mean')\n",
    "row_filter = lambda r: ~r.task.isin(['kddcup09_appetency', 'colleges'])\n",
    "\n",
    "definitions = dict(\n",
    "    constantpredictor=dict(\n",
    "        ref = True,\n",
    "        framework='constantpredictor_enc',\n",
    "        results_files=glob.glob(f\"{results_dir}/constantpredictor*/scores/results.csv\")\n",
    "    ),\n",
    "    autogluon=dict(\n",
    "        framework='AutoGluon',\n",
    "        results_files=glob.glob(f\"{results_dir}/autogluon*/scores/results.csv\")\n",
    "    ),\n",
    "    autosklearn=dict(\n",
    "        framework='autosklearn',\n",
    "        results_files=glob.glob(f\"{results_dir}/autosklearn*/scores/results.csv\")\n",
    "    ),\n",
    "    h2oautoml=dict(\n",
    "        framework='H2OAutoML',\n",
    "        results_files=glob.glob(f\"{results_dir}/h2oautoml*/scores/results.csv\")\n",
    "    ),\n",
    "    tpot=dict(\n",
    "        framework='TPOT',\n",
    "        results_files=glob.glob(f\"{results_dir}/tpot*/scores/results.csv\")\n",
    "    )\n",
    ")\n",
    "\n",
    "definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {k:v for k, v in definitions.items() \n",
    "        if (k in included_frameworks if included_frameworks else True) \n",
    "        and k not in excluded_frameworks}\n",
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_as_df(results_dict, row_filter=None):\n",
    "    def apply_filter(res, filtr):\n",
    "        r = res.results\n",
    "        return r.loc[filtr(r)]\n",
    "\n",
    "    if row_filter is None:\n",
    "        row_filter = lambda r: True\n",
    "\n",
    "    return pd.concat([apply_filter(res, lambda r: (r.framework==name) & row_filter(r)) \n",
    "                      for name, res in results_dict.items() \n",
    "                      if res is not None])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_results = {name: prepare_results(run['results_files'], \n",
    "                                     renamings={run['framework']: name},\n",
    "                                     exclusions=excluded_frameworks,\n",
    "                                     normalization=normalization,\n",
    "                                     duplicates_handling=duplicates_handling,\n",
    "                                     ) \n",
    "               for name, run in runs.items() if runs[name].get('ref', False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = next(res for res in ref_results.values()).metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_types = pd.DataFrame(m.__dict__ for m in metadata.values())['type'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_metadata(metadata, \n",
    "                filename=create_file(output_dir, \"datasets\", results_group, \"metadata.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref_res = results_as_df(ref_results, row_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_results = {name: prepare_results(run['results_files'], \n",
    "                                      renamings={run['framework']: name},\n",
    "                                      exclusions=excluded_frameworks,\n",
    "                                      imputation=imputation,\n",
    "                                      normalization=normalization,\n",
    "                                      ref_results=all_ref_res,\n",
    "                                      duplicates_handling=duplicates_handling\n",
    "                                      ) \n",
    "                for name, run in runs.items() if name not in ref_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = pd.concat([\n",
    "    all_ref_res, \n",
    "    results_as_df(runs_results, row_filter)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_summary = render_summary('result', \n",
    "                             results=all_res)\n",
    "res_summary.to_csv(create_file(output_dir, \"tables\", \"results_summary.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_summary = render_summary('score', \n",
    "                               results=all_res)\n",
    "score_summary.to_csv(create_file(output_dir, \"tables\", \"score_summary.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_summary = render_summary('models_count', \n",
    "                                results=all_res)\n",
    "models_summary.to_csv(create_file(output_dir, \"tables\", \"models_summary.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalization:\n",
    "    norm_score_summary = render_summary('norm_score', \n",
    "                                        results=all_res)\n",
    "    norm_score_summary.to_csv(create_file(output_dir, \"tables\", \"normalized_score_summary.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_leaderboard = render_leaderboard('score', \n",
    "                                           results=all_res,\n",
    "                                           aggregate=True)\n",
    "benchmark_leaderboard.to_csv(create_file(output_dir, \"tables\", \"benchmark_leaderboard.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'binary' in problem_types:\n",
    "    fig = draw_score_heatmap('score',\n",
    "                             results=all_res,\n",
    "                             type_filter='binary', \n",
    "                             metadata=metadata,\n",
    "                             x_labels=frameworks_labels or True,\n",
    "                             x_sort_by=frameworks_sort_key,\n",
    "                             y_sort_by='nrows',\n",
    "                             title=f\"Scores ({binary_score_label}) on {results_group} binary classification problems{title_extra}\",\n",
    "                             center=0.5\n",
    "                            );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"binary_score_heat.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'multiclass' in problem_types:\n",
    "    fig = draw_score_heatmap('score', \n",
    "                             results=all_res,\n",
    "                             type_filter='multiclass', \n",
    "                             metadata=metadata,\n",
    "                             x_labels=frameworks_labels  or True,\n",
    "                             x_sort_by=frameworks_sort_key,\n",
    "                             y_sort_by='nrows',\n",
    "                             title=f\"Scores ({multiclass_score_label}) on {results_group} multi-class classification problems{title_extra}\",\n",
    "                             center=0\n",
    "                            );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"multiclass_score_heat.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'regression' in problem_types:\n",
    "    fig = draw_score_heatmap('score', \n",
    "                             results=all_res,\n",
    "                             type_filter='regression', \n",
    "                             metadata=metadata,\n",
    "                             x_labels=frameworks_labels  or True,\n",
    "                             x_sort_by=frameworks_sort_key,\n",
    "                             y_sort_by='nrows',\n",
    "                             title=f\"Scores ({regression_score_label}) on {results_group} regression problems{title_extra}\",\n",
    "                             center=0\n",
    "                            );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"regression_score_heat.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_colormap(config.colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'binary' in problem_types:\n",
    "    fig = draw_score_barplot('score',\n",
    "                             results=all_res,\n",
    "                             type_filter='binary', \n",
    "                             metadata=metadata,\n",
    "                             x_sort_by=tasks_sort_by,\n",
    "                             ylabel=binary_score_label,\n",
    "                             ylim=dict(bottom=.5),\n",
    "                             hue_sort_by=frameworks_sort_key, \n",
    "                             ci=95,\n",
    "                             title=f\"Scores ({binary_score_label}) on {results_group} binary classification problems{title_extra}\",\n",
    "                             legend_loc='lower center',\n",
    "                             legend_labels=frameworks_labels,\n",
    "                            );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"binary_score_barplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'multiclass' in problem_types:\n",
    "    fig = draw_score_barplot('score',\n",
    "                             results=all_res,\n",
    "                             type_filter='multiclass', \n",
    "                             metadata=metadata,\n",
    "                             x_sort_by=tasks_sort_by,\n",
    "                             ylabel=multiclass_score_label,\n",
    "                             ylim=dict(top=0.1),\n",
    "                             hue_sort_by=frameworks_sort_key,\n",
    "                             ci=95,\n",
    "                             title=f\"Scores ({multiclass_score_label}) on {results_group} multiclass classification problems{title_extra}\",\n",
    "                             legend_loc='lower center',\n",
    "                             legend_labels=frameworks_labels,\n",
    "                            );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"multiclass_score_barplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'regression' in problem_types:\n",
    "    fig = draw_score_barplot('score',\n",
    "                             results=all_res,\n",
    "                             type_filter='regression', \n",
    "                             metadata=metadata,\n",
    "                             x_sort_by=tasks_sort_by,\n",
    "                             yscale='symlog',\n",
    "                             ylabel=regression_score_label,\n",
    "                             ylim=dict(top=0.1),\n",
    "                             hue_sort_by=frameworks_sort_key, \n",
    "                             ci=95,\n",
    "                             title=f\"Scores ({regression_score_label}) on {results_group} regression classification problems{title_extra}\",\n",
    "                             legend_loc='lower center',\n",
    "                             legend_labels=frameworks_labels,\n",
    "                             size=(8, 6),\n",
    "                            );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"regression_score_barplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'binary' in problem_types:\n",
    "    fig = draw_score_pointplot('score',\n",
    "                               results=all_res,\n",
    "                               type_filter='binary', \n",
    "                               metadata=metadata,\n",
    "                               x_sort_by=tasks_sort_by,\n",
    "                               ylabel=binary_score_label,\n",
    "                               ylim=dict(bottom=.5),\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               join='none', marker='hline_xspaced', ci=95, \n",
    "                               title=f\"Scores ({binary_score_label}) on {results_group} binary classification problems{title_extra}\",\n",
    "                               legend_loc='lower center',\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"binary_score_pointplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'multiclass' in problem_types:\n",
    "    fig = draw_score_pointplot('score',\n",
    "                               results=all_res,\n",
    "                               type_filter='multiclass', \n",
    "                               metadata=metadata,\n",
    "                               x_sort_by=tasks_sort_by,\n",
    "                               ylabel=multiclass_score_label,\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               join='none', marker='hline_xspaced', ci=95, \n",
    "                               title=f\"Scores ({multiclass_score_label}) on {results_group} multiclass classification problems{title_extra}\",\n",
    "                               legend_loc='lower center',\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"multiclass_score_pointplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'regression' in problem_types:\n",
    "    fig = draw_score_pointplot('score',\n",
    "                               results=all_res,\n",
    "                               type_filter='regression', \n",
    "                               metadata=metadata,\n",
    "                               x_sort_by=tasks_sort_by,\n",
    "                               ylabel=regression_score_label,\n",
    "                               yscale='symlog',\n",
    "                               ylim=dict(top=0.1),\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               join='none', marker='hline_xspaced', ci=95, \n",
    "                               title=f\"Scores ({regression_score_label}) on {results_group} regression classification problems{title_extra}\",\n",
    "                               legend_loc='lower center',\n",
    "                               legend_labels=frameworks_labels,\n",
    "                               size=(8, 6),\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"regression_score_pointplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'binary' in problem_types:\n",
    "    fig = draw_score_stripplot('score', \n",
    "                               results=all_res.sort_values(by=['framework']),\n",
    "                               type_filter='binary', \n",
    "                               metadata=metadata,\n",
    "                               xlabel=binary_score_label,\n",
    "                               y_sort_by=tasks_sort_by,\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               title=f\"Scores ({binary_score_label}) on {results_group} binary classification problems{title_extra}\",\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"binary_score_stripplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'multiclass' in problem_types:\n",
    "    fig = draw_score_stripplot('score', \n",
    "                               results=all_res.sort_values(by=['framework']),\n",
    "                               type_filter='multiclass', \n",
    "                               metadata=metadata,\n",
    "                               xlabel=multiclass_score_label,\n",
    "                               xscale='symlog',\n",
    "                               y_sort_by=tasks_sort_by,\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               title=f\"Scores ({multiclass_score_label}) on {results_group} multi-class classification problems{title_extra}\",\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"multiclass_score_stripplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'regression' in problem_types:\n",
    "    fig = draw_score_stripplot('score', \n",
    "                               results=all_res,\n",
    "                               type_filter='regression', \n",
    "                               metadata=metadata,\n",
    "                               xlabel=regression_score_label,\n",
    "                               xscale='symlog',\n",
    "                               y_sort_by=tasks_sort_by,\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               title=f\"Scores ({regression_score_label}) on {results_group} regression problems{title_extra}\",\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"regression_score_stripplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'binary' in problem_types and normalization:\n",
    "    fig = draw_score_stripplot('norm_score', \n",
    "                               results=all_res,\n",
    "                               type_filter='binary', \n",
    "                               metadata=metadata,\n",
    "                               xlabel=f\"rel. {binary_score_label}\",\n",
    "                               y_sort_by='nrows',\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               title=f\"Relative scores ({binary_score_label}) on {results_group} binary classification problems{title_extra}\",\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"binary_rel_score_stripplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'multiclass' in problem_types and normalization:\n",
    "    fig = draw_score_stripplot('norm_score', \n",
    "                               results=all_res,\n",
    "                               type_filter='multiclass', \n",
    "                               metadata=metadata,\n",
    "                               xlabel=f\"rel. {multiclass_score_label}\",\n",
    "                               xscale='symlog',\n",
    "                               y_sort_by='nrows',\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               title=f\"Relative scores ({multiclass_score_label}) on {results_group} multi-class classification problems{title_extra}\",\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"multiclass_rel_score_stripplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'regression' in problem_types and normalization:\n",
    "    fig = draw_score_stripplot('norm_score', \n",
    "                               results=all_res,\n",
    "                               type_filter='regression', \n",
    "                               metadata=metadata,\n",
    "                               xlabel=f\"rel. {regression_score_label}\",\n",
    "                               y_sort_by='nrows',\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               title=f\"Relative scores ({regression_score_label}) on {results_group} regression problems{title_extra}\",\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"regression_rel_score_stripplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlb",
   "language": "python",
   "name": "amlb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
