{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark results reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequirements\n",
    "This notebook requires a kernel running Python 3.5+.\n",
    "You can skip this section if the kernel is already configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r ./requirements.txt\n",
    "#!pip install jupyter_contrib_nbextensions\n",
    "#!jupyter contrib nbextension install --user\n",
    "#!jupyter nbextension enable python-markdown/main\n",
    "#!pip install jupyter_nbextensions_configurator\n",
    "#!jupyter nbextensions_configurator enable --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and selection of the results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "automlbenchmark_path = \"..\"\n",
    "amlb_dir = os.path.realpath(os.path.expanduser(automlbenchmark_path))\n",
    "for lib in [amlb_dir]:\n",
    "    sys.path.insert(0, lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amlb_report import draw_score_heatmap, draw_score_parallel_coord, draw_score_pointplot, draw_score_stripplot, draw_score_barplot\\\n",
    "                 , prepare_results, render_leaderboard, render_metadata, render_summary\n",
    "from amlb_report.util import create_file, display\n",
    "from amlb_report.visualizations.util import register_colormap, render_colormap, savefig\n",
    "import amlb_report.config as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading results, formatting and adding columns\n",
    "- `result` is the raw result metric computed from predictions at the end the benchmark.\n",
    "    For classification problems, it is usually `auc` for binomial classification and `neg_logloss` for multinomial classification (higher is always better).\n",
    "- `norm_result` is a normalization of `result` on a `[0, 1]` scale, with `{{normalization[0]}}` result as `0` and `{{normalization[1]}}` result as `1`.\n",
    "- `imp_result` for imputed results. Given a task and a framework:\n",
    "    - if **all folds results are missing**, then no imputation occurs, and the result is `nan` for each fold.\n",
    "    - if **only some folds results are missing**, then the missing result can be imputed by setting `{{imputation='framework'}}` and use that framework to impute the result for this fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! avoid editing this cell: custom config should be applied in the next cell.\n",
    "\n",
    "constraint = \"1h8c\"\n",
    "results_dir = \".\"\n",
    "output_dir = \".\"\n",
    "\n",
    "tasks_sort_by = 'nrows'\n",
    "results_group = ''\n",
    "\n",
    "included_frameworks = []\n",
    "excluded_frameworks = []\n",
    "frameworks_sort_key = None\n",
    "# frameworks_sort_key = lambda f: definitions[f]['key'] if 'key' in definitions[f] else f.lower()\n",
    "frameworks_labels = None\n",
    "# frameworks_labels = lambda l: definitions[l]['framework'].lower()\n",
    "duplicates_handling = 'fail' # accepted values: 'fail', 'keep_first', 'keep_last', 'keep_none'\n",
    "imputation = None\n",
    "normalization = None\n",
    "# normalization = (0, 'h2o', 'mean')\n",
    "row_filter = None\n",
    "# row_filter = lamdba r: r.fold == 0     #! r is a pd.Series\n",
    "title_extra = \"\"\n",
    "binary_result_label = 'AUC'\n",
    "multiclass_result_label = 'neg. Log loss'\n",
    "regression_result_label = 'neg. RMSE'\n",
    "\n",
    "# register_colormap(config.colormap, ('colorblind', [1, 0, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config and results definitions for current run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell  is an example showing how to use/customize this notebook depending on your results\n",
    "config.nfolds = 1\n",
    "\n",
    "results_dir = \"../results\"\n",
    "output_dir = \"./tmp\"\n",
    "duplicates_handling = 'keep_last'\n",
    "normalization = (0, 'constantpredictor', 'mean')  # normalizes results between 0 and constantpredictor\n",
    "# row_filter = lambda r: ~r.task.isin(['kddcup09_appetency', 'colleges'])\n",
    "\n",
    "definitions = dict(\n",
    "    constantpredictor=dict(\n",
    "        ref = True,\n",
    "        framework='constantpredictor_enc',\n",
    "        results=glob.glob(f\"{results_dir}/constantpredictor*/scores/results.csv\")\n",
    "    ),\n",
    "    autogluon=dict(\n",
    "        framework='AutoGluon',\n",
    "        results=glob.glob(f\"{results_dir}/autogluon*/scores/results.csv\")\n",
    "    ),\n",
    "    autosklearn=dict(\n",
    "        framework='autosklearn',\n",
    "        results=glob.glob(f\"{results_dir}/autosklearn*/scores/results.csv\")\n",
    "    ),\n",
    "    h2oautoml=dict(\n",
    "        framework='H2OAutoML',\n",
    "        results=glob.glob(f\"{results_dir}/h2oautoml*/scores/results.csv\")\n",
    "    ),\n",
    "    tpot=dict(\n",
    "        framework='TPOT',\n",
    "        results=glob.glob(f\"{results_dir}/tpot*/scores/results.csv\")\n",
    "    ),\n",
    "#     rf=dict(\n",
    "#         framework='RandomForest',\n",
    "#         results=my_results_df[my_results_df['framework']=='RandomForest']  # example showing that we can also use a dataframe (or its subset)\n",
    "#     )\n",
    ")\n",
    "\n",
    "#definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {k:v for k, v in definitions.items() \n",
    "        if (k in included_frameworks if included_frameworks else True) \n",
    "        and k not in excluded_frameworks}\n",
    "#runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_as_df(results_dict, row_filter=None):\n",
    "    def apply_filter(res, filtr):\n",
    "        r = res.results\n",
    "        return r.loc[filtr(r)]\n",
    "\n",
    "    if row_filter is None:\n",
    "        row_filter = lambda r: True\n",
    "\n",
    "    return pd.concat([apply_filter(res, lambda r: (r.framework==name) & row_filter(r)) \n",
    "                      for name, res in results_dict.items() \n",
    "                      if res is not None])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_results = {name: prepare_results(run['results'], \n",
    "                                     renamings={run['framework']: name},\n",
    "                                     exclusions=excluded_frameworks,\n",
    "                                     normalization=normalization,\n",
    "                                     duplicates_handling=duplicates_handling,\n",
    "                                     include_metadata=True\n",
    "                                     ) \n",
    "               for name, run in runs.items() if runs[name].get('ref', False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref_res = results_as_df(ref_results, row_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_results = {name: prepare_results(run['results'], \n",
    "                                      renamings={run['framework']: name},\n",
    "                                      exclusions=excluded_frameworks,\n",
    "                                      imputation=imputation,\n",
    "                                      normalization=normalization,\n",
    "                                      ref_results=all_ref_res,\n",
    "                                      duplicates_handling=duplicates_handling\n",
    "                                      ) \n",
    "                for name, run in runs.items() if name not in ref_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = pd.concat([\n",
    "    all_ref_res, \n",
    "    results_as_df(runs_results, row_filter)\n",
    "])\n",
    "all_results = {**ref_results, **runs_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "metadata = reduce(lambda l, r: {**r, **l}, \n",
    "                  [res.metadata \n",
    "                   for res in list(ref_results.values())+list(runs_results.values()) \n",
    "                   if res is not None],\n",
    "                  {})\n",
    "# metadata = next(res for res in ref_results.values()).metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_types = pd.DataFrame(m.__dict__ for m in metadata.values())['type'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_res = pd.concat([r.done.reset_index() for r in all_results.values() if r is not None])\n",
    "merged_res = merged_res[merged_res['id'].notna()]\n",
    "merged_results = prepare_results(merged_res)\n",
    "\n",
    "def render_tasks_by_state(state='done'):\n",
    "#     tasks = pd.concat([getattr(r, state).reset_index()\n",
    "#                         .groupby(['task', 'framework'])['fold']\n",
    "#                         .unique()\n",
    "#                        for r in all_results.values()\n",
    "#                        if r is not None])\n",
    "    tasks = (getattr(merged_results, state).reset_index()\n",
    "                .groupby(['task', 'framework'])['fold']\n",
    "                .unique())\n",
    "    display(tasks, pretty=True)\n",
    "    # display(tabulate(done, tablefmt='plain'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_metadata(metadata, \n",
    "                filename=create_file(output_dir, \"datasets\", results_group, \"metadata.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completed tasks/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "render_tasks_by_state('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing or crashed/aborted tasks/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "render_tasks_by_state('missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failing tasks/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "render_tasks_by_state('failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = (merged_results.failed.groupby(['task', 'fold', 'framework'])['info']\n",
    "                          .unique())\n",
    "display(failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amlb_report.analysis import list_outliers\n",
    "\n",
    "display(list_outliers('result', \n",
    "                      results=merged_results.results,\n",
    "#                       results=merged_results.loc[merged_results.framework=='h2oautoml']\n",
    "                      z_threshold=2.5,\n",
    "                     ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging using arithmetic mean over fold `result`.\n",
    "In following summaries, if not mentioned otherwise, and if results imputation was enabled, the means are computed over imputed results .\n",
    "Given a task and a framework:\n",
    "- if **all folds results are missing**, then no imputation occured, and the mean result is `nan`.\n",
    "- if **only some folds results are missing**, then the amount of imputed results that contributed to the mean are displayed between parenthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of models trained\n",
    "\n",
    "When available, displays the average amount of models trained by the framework for each dataset.\n",
    "\n",
    "This amount should be interpreted differently for each framework.\n",
    "For example, with *RandomForest*, this amount corresponds to the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_summary = render_summary('models_count', \n",
    "                                results=all_res)\n",
    "models_summary.to_csv(create_file(output_dir, \"tables\", \"models_summary.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuls mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_summary = render_summary('result', \n",
    "                             results=all_res)\n",
    "res_summary.to_csv(create_file(output_dir, \"tables\", \"results_summary.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalization:\n",
    "    norm_result_summary = render_summary('norm_result', \n",
    "                                         results=all_res)\n",
    "    norm_result_summary.to_csv(create_file(output_dir, \"tables\", \"normalized_result_summary.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_leaderboard = render_leaderboard('result', \n",
    "                                           results=all_res,\n",
    "                                           aggregate=True)\n",
    "benchmark_leaderboard.to_csv(create_file(output_dir, \"tables\", \"benchmark_leaderboard.csv\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "folds_leaderboard = render_leaderboard('result', \n",
    "                                        results=all_res,\n",
    "                                        aggregate=False)\n",
    "folds_leaderboard.to_csv(create_file(output_dir, \"tables\", \"folds_leaderboard.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_colormap(config.colormap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'binary' in problem_types:\n",
    "    fig = draw_score_heatmap('result',\n",
    "                             results=all_res,\n",
    "                             type_filter='binary', \n",
    "                             metadata=metadata,\n",
    "                             x_labels=frameworks_labels or True,\n",
    "                             x_sort_by=frameworks_sort_key,\n",
    "                             y_sort_by='nrows',\n",
    "                             title=f\"Results ({binary_result_label}) on {results_group} binary classification problems{title_extra}\",\n",
    "                             center=0.5\n",
    "                            );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"binary_result_heat.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'multiclass' in problem_types:\n",
    "    fig = draw_score_heatmap('result', \n",
    "                             results=all_res,\n",
    "                             type_filter='multiclass', \n",
    "                             metadata=metadata,\n",
    "                             x_labels=frameworks_labels  or True,\n",
    "                             x_sort_by=frameworks_sort_key,\n",
    "                             y_sort_by='nrows',\n",
    "                             title=f\"Results ({multiclass_result_label}) on {results_group} multi-class classification problems{title_extra}\",\n",
    "                             center=0\n",
    "                            );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"multiclass_result_heat.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'regression' in problem_types:\n",
    "    fig = draw_score_heatmap('result', \n",
    "                             results=all_res,\n",
    "                             type_filter='regression', \n",
    "                             metadata=metadata,\n",
    "                             x_labels=frameworks_labels  or True,\n",
    "                             x_sort_by=frameworks_sort_key,\n",
    "                             y_sort_by='nrows',\n",
    "                             title=f\"Results ({regression_result_label}) on {results_group} regression problems{title_extra}\",\n",
    "                             center=0\n",
    "                            );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"regression_result_heat.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'binary' in problem_types:\n",
    "    fig = draw_score_barplot('result',\n",
    "                             results=all_res,\n",
    "                             type_filter='binary', \n",
    "                             metadata=metadata,\n",
    "                             x_sort_by=tasks_sort_by,\n",
    "                             ylabel=binary_result_label,\n",
    "                             ylim=dict(bottom=.5),\n",
    "                             hue_sort_by=frameworks_sort_key, \n",
    "                             ci=95,\n",
    "                             title=f\"Results ({binary_result_label}) on {results_group} binary classification problems{title_extra}\",\n",
    "                             legend_loc='lower center',\n",
    "                             legend_labels=frameworks_labels,\n",
    "                            );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"binary_result_barplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'multiclass' in problem_types:\n",
    "    fig = draw_score_barplot('result',\n",
    "                             results=all_res,\n",
    "                             type_filter='multiclass', \n",
    "                             metadata=metadata,\n",
    "                             x_sort_by=tasks_sort_by,\n",
    "                             ylabel=multiclass_result_label,\n",
    "                             ylim=dict(top=0.1),\n",
    "                             hue_sort_by=frameworks_sort_key,\n",
    "                             ci=95,\n",
    "                             title=f\"Results ({multiclass_result_label}) on {results_group} multiclass classification problems{title_extra}\",\n",
    "                             legend_loc='lower center',\n",
    "                             legend_labels=frameworks_labels,\n",
    "                            );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"multiclass_result_barplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'regression' in problem_types:\n",
    "    fig = draw_score_barplot('result',\n",
    "                             results=all_res,\n",
    "                             type_filter='regression', \n",
    "                             metadata=metadata,\n",
    "                             x_sort_by=tasks_sort_by,\n",
    "                             yscale='symlog',\n",
    "                             ylabel=regression_result_label,\n",
    "                             ylim=dict(top=0.1),\n",
    "                             hue_sort_by=frameworks_sort_key, \n",
    "                             ci=95,\n",
    "                             title=f\"Results ({regression_result_label}) on {results_group} regression classification problems{title_extra}\",\n",
    "                             legend_loc='lower center',\n",
    "                             legend_labels=frameworks_labels,\n",
    "                             size=(8, 6),\n",
    "                            );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"regression_result_barplot.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'binary' in problem_types:\n",
    "    fig = draw_score_pointplot('result',\n",
    "                               results=all_res,\n",
    "                               type_filter='binary', \n",
    "                               metadata=metadata,\n",
    "                               x_sort_by=tasks_sort_by,\n",
    "                               ylabel=binary_result_label,\n",
    "                               ylim=dict(bottom=.5),\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               join='none', marker='hline_xspaced', ci=95, \n",
    "                               title=f\"Results ({binary_result_label}) on {results_group} binary classification problems{title_extra}\",\n",
    "                               legend_loc='lower center',\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"binary_result_pointplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'multiclass' in problem_types:\n",
    "    fig = draw_score_pointplot('result',\n",
    "                               results=all_res,\n",
    "                               type_filter='multiclass', \n",
    "                               metadata=metadata,\n",
    "                               x_sort_by=tasks_sort_by,\n",
    "                               ylabel=multiclass_result_label,\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               join='none', marker='hline_xspaced', ci=95, \n",
    "                               title=f\"Results ({multiclass_result_label}) on {results_group} multiclass classification problems{title_extra}\",\n",
    "                               legend_loc='lower center',\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"multiclass_result_pointplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'regression' in problem_types:\n",
    "    fig = draw_score_pointplot('result',\n",
    "                               results=all_res,\n",
    "                               type_filter='regression', \n",
    "                               metadata=metadata,\n",
    "                               x_sort_by=tasks_sort_by,\n",
    "                               ylabel=regression_result_label,\n",
    "                               yscale='symlog',\n",
    "                               ylim=dict(top=0.1),\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               join='none', marker='hline_xspaced', ci=95, \n",
    "                               title=f\"Results ({regression_result_label}) on {results_group} regression classification problems{title_extra}\",\n",
    "                               legend_loc='lower center',\n",
    "                               legend_labels=frameworks_labels,\n",
    "                               size=(8, 6),\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"regression_result_pointplot.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'binary' in problem_types:\n",
    "    fig = draw_score_stripplot('result', \n",
    "                               results=all_res.sort_values(by=['framework']),\n",
    "                               type_filter='binary', \n",
    "                               metadata=metadata,\n",
    "                               xlabel=binary_result_label,\n",
    "                               y_sort_by=tasks_sort_by,\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               title=f\"Results ({binary_result_label}) on {results_group} binary classification problems{title_extra}\",\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"binary_result_stripplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'multiclass' in problem_types:\n",
    "    fig = draw_score_stripplot('result', \n",
    "                               results=all_res.sort_values(by=['framework']),\n",
    "                               type_filter='multiclass', \n",
    "                               metadata=metadata,\n",
    "                               xlabel=multiclass_result_label,\n",
    "                               xscale='symlog',\n",
    "                               y_sort_by=tasks_sort_by,\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               title=f\"Results ({multiclass_result_label}) on {results_group} multi-class classification problems{title_extra}\",\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"multiclass_result_stripplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'regression' in problem_types:\n",
    "    fig = draw_score_stripplot('result', \n",
    "                               results=all_res,\n",
    "                               type_filter='regression', \n",
    "                               metadata=metadata,\n",
    "                               xlabel=regression_result_label,\n",
    "                               xscale='symlog',\n",
    "                               y_sort_by=tasks_sort_by,\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               title=f\"Results ({regression_result_label}) on {results_group} regression problems{title_extra}\",\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"regression_result_stripplot.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized strip plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'binary' in problem_types and normalization:\n",
    "    fig = draw_score_stripplot('norm_result', \n",
    "                               results=all_res,\n",
    "                               type_filter='binary', \n",
    "                               metadata=metadata,\n",
    "                               xlabel=f\"rel. {binary_result_label}\",\n",
    "                               y_sort_by='nrows',\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               title=f\"Relative results ({binary_result_label}) on {results_group} binary classification problems{title_extra}\",\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"binary_rel_result_stripplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'multiclass' in problem_types and normalization:\n",
    "    fig = draw_score_stripplot('norm_result', \n",
    "                               results=all_res,\n",
    "                               type_filter='multiclass', \n",
    "                               metadata=metadata,\n",
    "                               xlabel=f\"rel. {multiclass_result_label}\",\n",
    "                               xscale='symlog',\n",
    "                               y_sort_by='nrows',\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               title=f\"Relative results ({multiclass_result_label}) on {results_group} multi-class classification problems{title_extra}\",\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"multiclass_rel_result_stripplot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'regression' in problem_types and normalization:\n",
    "    fig = draw_score_stripplot('norm_result', \n",
    "                               results=all_res,\n",
    "                               type_filter='regression', \n",
    "                               metadata=metadata,\n",
    "                               xlabel=f\"rel. {regression_result_label}\",\n",
    "                               y_sort_by='nrows',\n",
    "                               hue_sort_by=frameworks_sort_key,\n",
    "                               title=f\"Relative results ({regression_result_label}) on {results_group} regression problems{title_extra}\",\n",
    "                               legend_labels=frameworks_labels,\n",
    "                              );\n",
    "    savefig(fig, create_file(output_dir, \"visualizations\", \"regression_rel_result_stripplot.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlb",
   "language": "python",
   "name": "amlb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
