{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark results reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequirements\n",
    "This notebook requires a kernel running Python 3.5+.\n",
    "You can skip this section if the kernel is already configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install openml\n",
    "!pip install tabulate\n",
    "#!pip install jupyter_contrib_nbextensions\n",
    "#!jupyter contrib nbextension install --user\n",
    "#!jupyter nbextension enable python-markdown/main\n",
    "#!pip install jupyter_nbextensions_configurator\n",
    "#!jupyter nbextensions_configurator enable --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and selection of the results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython import display as ipyd\n",
    "from IPython.display import FileLink, FileLinks\n",
    "import functools as ft\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import scipy as sp\n",
    "import seaborn as sb\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#disabling this cell\n",
    "results_dir = \"./reports\"\n",
    "print(\"current working dir: {}\".format(os.getcwd()))\n",
    "try:\n",
    "    os.chdir(results_dir)\n",
    "except:\n",
    "    pass\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters\n",
    "Global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nfolds = 10\n",
    "ff = '%.6g'\n",
    "colormap = 'tab10'\n",
    "# colormap = 'Set2'\n",
    "# colormap = 'Dark2'\n",
    "\n",
    "renamings = dict(\n",
    "    constantpredictor_enc='constantpredictor'\n",
    ")\n",
    "excluded_frameworks = ['oboe']\n",
    "binary_score_label = 'AUC'\n",
    "multiclass_score_label = 'logloss'\n",
    "\n",
    "# impute_missing_with = 'constantpredictor'\n",
    "impute_missing_with = 'randomforest'\n",
    "zero_one_refs = ('constantpredictor', 'tunedrandomforest')\n",
    "\n",
    "all_results_files = {\n",
    "    'old': [\n",
    "        \"results_valid_ref.csv\", \"results_valid.csv\",\n",
    "        \"results_small-2c1h_ref.csv\", \"results_small-2c1h.csv\",\n",
    "        \"results_medium-4c1h_ref.csv\", \"results_medium-4c1h.csv\",\n",
    "        \"results_medium-4c4h_ref.csv\", \"results_medium-4c4h.csv\",\n",
    "    ],\n",
    "    '1h': [\n",
    "        \"results_small-8c1h_ref.csv\", \"results_small-8c1h.csv\",\n",
    "        \"results_medium-8c1h_ref.csv\", \"results_medium-8c1h.csv\",            \n",
    "    ],\n",
    "    '4h': [\n",
    "        \"results_small-8c4h_ref.csv\", \"results_small-8c4h.csv\",\n",
    "        \"results_medium-8c4h_ref.csv\", \"results_medium-8c4h.csv\",    \n",
    "        \"results_large-8c4h_ref.csv\", \"results_large-8c4h.csv\",       \n",
    "    ],\n",
    "    '8h': [\n",
    "        \"results_large-8c8h_ref.csv\", \"results_large-8c8h.csv\",        \n",
    "    ]\n",
    "}\n",
    "\n",
    "results_group = '1h'\n",
    "results_files = all_results_files[results_group]\n",
    "tasks_sort_by = 'nrows'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading results, formatting and adding columns\n",
    "- `result` is the raw result metric computed from predictions at the end the benchmark.\n",
    "    For classification problems, it is usually `auc` for binomial classification and `logloss` for multinomial classification.\n",
    "- `score` ensures a standard comparison between tasks: **higher is always better**.\n",
    "- `norm_score` is a normalization of `score` on a `[0, 1]` scale, with `{{zero_one_refs[0]}}` score as `0` and `{{zero_one_refs[1]}}` score as `1`.\n",
    "- `imp_result` and `imp_score` for imputed results/scores. Given a task and a framework:\n",
    "    - if **all folds results/scores are missing**, then no imputation occurs, and the result is `nan` for each fold.\n",
    "    - if **only some folds results/scores are missing**, then the missing result is imputed by the `{{impute_missing_with}}` result for this fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global utility functions\n",
    "\n",
    "class Namespace:  \n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(**kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.__dict__)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.__dict__)\n",
    "\n",
    "    def extend(self, **kwargs):\n",
    "        clone = Namespace(**self.__dict__)\n",
    "        clone.__dict__.update(**kwargs)\n",
    "        return clone\n",
    "\n",
    "def create_file(*path_tokens):\n",
    "    path = os.path.realpath(os.path.join(*path_tokens))\n",
    "    if not os.path.exists(path):\n",
    "        dirname, basename = os.path.split(path)\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname, exist_ok=True)\n",
    "        if basename:\n",
    "            open(path, 'a').close()\n",
    "    return path\n",
    "\n",
    "def display(fr, pretty=True, float_format=ff):\n",
    "    with pd.option_context(\n",
    "        'display.max_rows', len(fr), \n",
    "        'display.float_format', lambda f: float_format % f\n",
    "        ):\n",
    "        if type(fr) is pd.Series:\n",
    "            fr = fr.to_frame()\n",
    "        if pretty and type(fr) is pd.DataFrame:\n",
    "            fr.style.set_properties(**{'vertical-align':'top'})\n",
    "            ipyd.display(ipyd.HTML(fr.to_html()))\n",
    "        else:\n",
    "            print(fr)\n",
    "\n",
    "\n",
    "def sort_dataframe(df, by=None, axis=0):\n",
    "    if axis == 1:\n",
    "        cols = [col for col in df.columns]\n",
    "        cols.sort(key=by) if by else cols.sort()\n",
    "        return df[cols]\n",
    "#         return df.sort_index(by, axis=1)\n",
    "    else:\n",
    "        if by:\n",
    "            tmp_sort='tmp_sort'\n",
    "            tmp_df = df.reset_index()\n",
    "            tmp_df = tmp_df.assign(tmp_sort=by)\n",
    "            tmp_df.set_index([*df.index.names, tmp_sort], inplace=True)\n",
    "            tmp_df.sort_index(level=tmp_sort, inplace=True)\n",
    "            tmp_df.set_index(tmp_df.index.droplevel(tmp_sort), inplace=True)\n",
    "            return tmp_df\n",
    "        return df.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml as oml\n",
    "\n",
    "\n",
    "def dataset_metadata(task_id):\n",
    "#     print(f\"loading {task_id}\")\n",
    "    tid = int(task_id.split(\"/\")[2]) if task_id.startswith('openml.org') else int(task_id)\n",
    "    task = oml.tasks.get_task(task_id=tid, download_data=False)\n",
    "    dataset = oml.datasets.get_dataset(task.dataset_id, download_data=False)\n",
    "    did = dataset.dataset_id\n",
    "    name = dataset.name\n",
    "    nrows = int(dataset.qualities['NumberOfInstances'])\n",
    "    nfeatures = int(dataset.qualities['NumberOfFeatures'])\n",
    "    nclasses = int(dataset.qualities['NumberOfClasses'])\n",
    "    task_type = ('regression' if nclasses == 0 \n",
    "                 else 'binary' if nclasses == 2 \n",
    "                 else 'multiclass' if nclasses > 2 \n",
    "                 else 'unknown')\n",
    "#     print(f\"loaded {name}\")\n",
    "    return Namespace(\n",
    "        task=f\"openml.org/t/{tid}\",\n",
    "        dataset=f\"openml.org/d/{did}\",\n",
    "        type=task_type,\n",
    "        name=name,\n",
    "        nrows=nrows,\n",
    "        nfeatures=nfeatures,\n",
    "        nclasses=nclasses,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_dataset_metadata(results):\n",
    "    tids = results.id.unique()\n",
    "    # task names are hardcoded in benchmark definitions, so we need to map them with their task id\n",
    "    lookup_df = results.filter(items=['id', 'task'], axis=1).drop_duplicates()\n",
    "    lookup_map = {rec['id']:rec['task'] for rec in lookup_df.to_dict('records')}\n",
    "#     print(lookup_map)\n",
    "    metadata = {lookup_map[m.task]:m for m in [dataset_metadata(tid) for tid in tids]}\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def save_metadata(metadata, filename='metadata.csv'):\n",
    "    df = pd.DataFrame([m.__dict__ for m in metadata.values()], \n",
    "                      columns=['task', 'name', 'type', 'dataset', 'nrows', 'nfeatures', 'nclasses'])\n",
    "    df.sort_values(by='name', inplace=True)\n",
    "    df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# results loading + processing functions\n",
    "\n",
    "def load_results(files):\n",
    "    return pd.concat([pd.read_csv(file) for file in files], ignore_index=True)\n",
    "\n",
    "                \n",
    "def task_prop(row, metadata, prop):\n",
    "    return getattr(metadata.get(row.task), prop)\n",
    "\n",
    "\n",
    "def impute_result(row, results_df, res_col='result', ref_framework=impute_missing_with):\n",
    "    if pd.notna(row[res_col]):\n",
    "        return row[res_col]\n",
    "    # if all folds are failed or missing, don't impute\n",
    "    if pd.isna(results_df.loc[(results_df.task==row.task)&(results_df.framework==row.framework)][res_col]).all():\n",
    "        return np.nan\n",
    "    # impute with ref framework corresponding value\n",
    "    return (results_df.loc[(results_df.framework==ref_framework)\n",
    "                           &(results_df.task==row.task)\n",
    "                           &(results_df.fold==row.fold)][res_col]\n",
    "                     .item())\n",
    "\n",
    "\n",
    "def imputed(row):\n",
    "    return pd.isna(row.result) and pd.notna(row.imp_result)\n",
    "    \n",
    "\n",
    "def score(row, res_col='result'):\n",
    "    return row[res_col] if row[res_col] in [row.auc, row.acc]\\\n",
    "                        else - row[res_col]\n",
    "\n",
    "\n",
    "def norm_score(row, results_df, score_col='score', zero_one_refs=zero_one_refs):\n",
    "    zero, one = (results_df.loc[(results_df.framework==ref)\n",
    "                                &(results_df.task==row.task)\n",
    "                                &(results_df.fold==row.fold)][score_col]\n",
    "                           .item()\n",
    "                 for ref in zero_one_refs)\n",
    "    return (row[score_col] - zero) / (one - zero)\n",
    " \n",
    "    \n",
    "def sorted_ints(arr): \n",
    "    return sorted(list(map(int, arr[~np.isnan(arr)])))\n",
    "\n",
    "\n",
    "def prepare_results(results_files, exclude=excluded_frameworks):\n",
    "    results = load_results(results_files).replace(renamings)\n",
    "    if exclude:\n",
    "        results = results.loc[~results.framework.isin(exclude)]\n",
    "    results.task = results.task.str.lower()\n",
    "    results.framework = results.framework.str.lower()\n",
    "    results.fold = results.fold.apply(int)\n",
    "\n",
    "    frameworks = results.framework.unique()\n",
    "    frameworks.sort()\n",
    "    \n",
    "    tasks = results.task.unique()\n",
    "    tasks.sort()\n",
    "    \n",
    "    folds = results.fold.unique()\n",
    "    \n",
    "    metadata = load_dataset_metadata(results)\n",
    "\n",
    "    done = results.set_index(['task', 'fold', 'framework'])\n",
    "    if not done.index.is_unique:\n",
    "        print(\"Duplicate entries:\")\n",
    "        display(done[done.index.duplicated(keep=False)]\n",
    "                    .sort_values(by=done.index.names), \n",
    "                pretty=False)\n",
    "    assert done.index.is_unique\n",
    "    \n",
    "    missing = (pd.DataFrame([(task, fold, framework, 'missing') \n",
    "                             for task in tasks \n",
    "                             for fold in range(nfolds)\n",
    "                             for framework in frameworks \n",
    "                             if (task, fold, framework) not in done.index],\n",
    "                            columns=[*done.index.names, 'info'])\n",
    "                 .set_index(done.index.names))\n",
    "    assert missing.index.is_unique\n",
    "    failed = (results.loc[pd.notna(results['info'])]\n",
    "                     .set_index(done.index.names))\n",
    "    assert failed.index.is_unique\n",
    "\n",
    "    # extending the data frame \n",
    "    results = results.append(missing.reset_index())\n",
    "    results['type'] = [task_prop(row, metadata, 'type') for _, row in results.iterrows()]\n",
    "    results['score'] = [score(row) for _, row in results.iterrows()]\n",
    "    results['imp_result'] = [impute_result(row, results) for _, row in results.iterrows()]\n",
    "    results['imp_score'] = [impute_result(row, results, 'score') for _, row in results.iterrows()]\n",
    "    results['norm_score'] = [norm_score(row, results, 'imp_score') for _, row in results.iterrows()]\n",
    "\n",
    "    return Namespace(\n",
    "        results=results,\n",
    "        frameworks=frameworks,\n",
    "        tasks=tasks,\n",
    "        folds=folds,\n",
    "        metadata=metadata,\n",
    "        done=done,\n",
    "        missing=missing,\n",
    "        failed=failed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load result files\n",
    "res = prepare_results(results_files)\n",
    "res.results.to_csv(create_file(\"tables\", results_group, \"all_results.csv\"), \n",
    "                   index=False, \n",
    "                   float_format=ff)\n",
    "save_metadata(res.metadata, filename=create_file(\"datasets\", results_group, \"metadata.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tasks = (res.results.groupby(['task', 'type'])['id']\n",
    "                    .unique()\n",
    "                    .map(lambda id: id[0]))\n",
    "display(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completed tasks/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "done = (res.done.reset_index()\n",
    "                .groupby(['task', 'framework'])['fold']\n",
    "                .unique())\n",
    "display(done, pretty=False)\n",
    "# display(tabulate(done, tablefmt='plain'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing or crashed/aborted tasks/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# not_done = pd.DataFrame([(task, framework) for task in res.tasks \n",
    "#                                            for framework in res.frameworks \n",
    "#                                            if (task, framework) not in done.index],\n",
    "#                         columns=['task', 'framework'])\n",
    "# missing = res.results.append(not_done)\\\n",
    "#                      .groupby(['task', 'framework'])['fold']\\\n",
    "#                      .unique()\\\n",
    "#                      .map(sorted_ints)\\\n",
    "#                      .map(lambda arr: sorted(list(set(range(0, nfolds)) - set(arr))))\\\n",
    "#                      .where(lambda values: values.map(lambda arr: len(arr) > 0))\\\n",
    "#                      .dropna()\n",
    "\n",
    "missing = (res.missing.reset_index()\n",
    "                      .groupby(['task', 'framework'])['fold']\n",
    "                      .unique())\n",
    "display(missing, pretty=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failing tasks/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# failed = res.results.where(np.isnan(all_results.result))\\\n",
    "#                     .groupby(['task', 'framework'])['fold']\\\n",
    "#                     .unique()\\\n",
    "#                     .map(sorted_ints)\n",
    "\n",
    "failed = (res.failed.reset_index()\n",
    "                    .groupby(['task', 'framework'])['fold']\n",
    "                    .unique())\n",
    "display(failed, pretty=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def list_outliers(col, results, z_threshold=3):\n",
    "    df = results.pivot_table(index=['type','task', 'framework'], columns='fold', values=col)\n",
    "    df_mean = df.mean(axis=1)\n",
    "    df_std = df.std(axis=1)\n",
    "    z_score = (df.sub(df_mean, axis=0)\n",
    "                 .div(df_std, axis=0)\n",
    "                 .abs())\n",
    "    return z_score.where(z_score > z_threshold).dropna(axis=0, how='all')\n",
    "    \n",
    "display(list_outliers('result', \n",
    "                      results=res.results,\n",
    "#                       results=res.results.loc[res.results.framework=='h2oautoml']\n",
    "                      z_threshold=2.5,\n",
    "                     ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common functions for data reports\n",
    "\n",
    "def add_imputed_mark(values, imp, val_type=float, val_format=None):\n",
    "    formats = dict(float=\"{:,.6g}{}\", int=\"{0:d}{}\", str=\"{}{}\")\n",
    "    format_value = (val_format if val_format is not None\n",
    "                               else lambda *val: formats[val_type.__name__].format(*val))\n",
    "    return (values.astype(object)\n",
    "                  .combine(imp, \n",
    "                           lambda val, imp: format_value(val, \" ({:.0g})\".format(imp) if imp else '')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging using arithmetic mean over fold `result` or `score`.\n",
    "In following summaries, if not mentioned otherwise, the means are computed over imputed results/scores.\n",
    "Given a task and a framework:\n",
    "- if **all folds results/scores are missing**, then no imputation occured, and the mean result is `nan`.\n",
    "- if **only some folds results/scores are missing**, then the amount of imputed results that contributed to the mean are displayed between parenthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def render_summary(col, results, show_imputations=True, filename=None, float_format=ff):\n",
    "    res_group = results.groupby(['type', 'task', 'framework'])\n",
    "    df = res_group[col].mean().unstack()\n",
    "    if show_imputations:\n",
    "        imputed_df = (res_group['result', 'imp_result']\n",
    "                          .apply(lambda df: sum(imputed(row) for _, row in df.iterrows()))\n",
    "                          .unstack())    \n",
    "        df = df.combine(imputed_df, ft.partial(add_imputed_mark, \n",
    "                                               val_format=lambda *v: (float_format+\"%s\") % tuple(v)))\n",
    "    display(df, float_format=float_format)\n",
    "    if filename is not None:\n",
    "        df.to_csv(create_file(\"tables\", results_group, filename), float_format=float_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_results = res.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of models trained\n",
    "\n",
    "When available, displays the average amount of models trained by the framework for each dataset.\n",
    "\n",
    "This amount should be interpreted differently for each framework.\n",
    "For example, with *RandomForest*, this amount corresponds to the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_summary('models', \n",
    "               results=summary_results, \n",
    "               filename=\"models_summary.csv\", \n",
    "               float_format=\"%.f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "render_summary('result', \n",
    "               results=summary_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_summary('imp_result', \n",
    "               results=summary_results,\n",
    "               filename=\"result_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_summary('imp_score', \n",
    "               results=summary_results,\n",
    "               filename=\"score_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_summary('norm_score', \n",
    "               results=summary_results,\n",
    "               filename=\"norm_score_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def rank(scores):\n",
    "    sorted_scores = pd.Series(scores.unique()).sort_values(ascending=False)\n",
    "    ranks = pd.Series(index=scores.index)\n",
    "    for idx, value in scores.items():\n",
    "        try:\n",
    "            ranks.at[idx] = np.where(sorted_scores == value)[0][0]+1\n",
    "        except IndexError:\n",
    "            ranks.at[idx] = np.nan\n",
    "    return ranks\n",
    "\n",
    "def render_leaderboard(col, results, aggregate=False, show_imputations=False, filename=None):\n",
    "    res_group = results.groupby(['type', 'task', 'framework'])\n",
    "    df = (res_group[col].mean().unstack() if aggregate \n",
    "          else results.pivot_table(index=['type','task', 'fold'], columns='framework', values=col))\n",
    "    df = (df.apply(rank, axis=1, result_type='broadcast')\n",
    "            .astype(object)) \n",
    "    if show_imputations:\n",
    "        imputed_df = (res_group['result', 'imp_result']\n",
    "                          .apply(lambda df: sum(imputed(row) for _, row in df.iterrows()))\n",
    "                          .unstack())    \n",
    "        df = df.combine(imputed_df, add_imputed_mark)\n",
    "    display(df)\n",
    "    if filename is not None:\n",
    "        df.to_csv(create_file(\"tables\", results_group, filename), float_format='%.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_results = res.results.loc[~res.results.framework.isin(['constantpredictor', 'randomforest'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_leaderboard('imp_score', \n",
    "                   results=leaderboard_results,\n",
    "                   aggregate=True, \n",
    "                   show_imputations=True, \n",
    "                   filename=\"tasks_leaderboard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folds leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "render_leaderboard('score', \n",
    "                   results=res.results,\n",
    "                   filename=\"folds_leaderboard.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# common functions for visualizations\n",
    "\n",
    "def savefig(fig, path):\n",
    "    fig.savefig(path, bbox_inches='tight')\n",
    "    \n",
    "\n",
    "def task_labels(index):\n",
    "    max_length = 16\n",
    "    return (index.droplevel('type')\n",
    "                .map(lambda x: x if len(x) <= max_length else u'{}â€¦'.format(x[:max_length-1]))\n",
    "                .values)\n",
    "\n",
    "def set_labels(axes, \n",
    "               title=None,\n",
    "               xlabel=None, ylabel=None,\n",
    "               x_labels=None, y_labels=None,\n",
    "               x_tick_params=None, y_tick_params=None,\n",
    "               legend_title=None):\n",
    "    \n",
    "    axes.set_title('' if not title else title, fontsize='xx-large')\n",
    "    axes.set_xlabel('' if not xlabel else xlabel, fontsize='x-large')\n",
    "    axes.set_ylabel('' if not ylabel else ylabel, fontsize='x-large')\n",
    "    if not x_tick_params:\n",
    "        x_tick_params = {}\n",
    "    if not y_tick_params:\n",
    "        y_tick_params = {}\n",
    "    axes.tick_params(axis='x', labelsize='x-large', **x_tick_params)\n",
    "    axes.tick_params(axis='y', labelsize='x-large', **y_tick_params)\n",
    "    if x_labels is not None:\n",
    "        axes.set_xticklabels(x_labels)\n",
    "    if y_labels is not None:\n",
    "        axes.set_yticklabels(y_labels)\n",
    "    legend = axes.get_legend()\n",
    "    if legend:\n",
    "        legend_title = legend_title or legend.get_title().get_text()\n",
    "        legend.set_title(legend_title, prop=dict(size='x-large'))\n",
    "        for text in legend.get_texts():\n",
    "            text.set_fontsize('x-large')\n",
    "            \n",
    "def set_scales(axes, xscale=None, yscale=None):\n",
    "    if isinstance(xscale, str):\n",
    "        axes.set_xscale(xscale)\n",
    "    elif isinstance(xscale, tuple):\n",
    "        axes.set_xscale(xscale[0], **xscale[1])\n",
    "    if isinstance(yscale, str):\n",
    "        axes.set_yscale(yscale)\n",
    "    elif isinstance(yscale, tuple):\n",
    "        axes.set_yscale(yscale[0], **yscale[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def draw_heatmap(df, \n",
    "                 x_labels=True, y_labels=True, \n",
    "                 title=None, xlabel=None, ylabel=None,\n",
    "                 **kwargs):\n",
    "    with sb.axes_style('white'), sb.plotting_context('paper'):\n",
    "#         print(sb.axes_style())\n",
    "#         print(sb.plotting_context())\n",
    "        axes = sb.heatmap(df, xticklabels=x_labels, yticklabels=y_labels,\n",
    "                          annot=True, cmap='RdYlGn', robust=True,\n",
    "                          **kwargs)\n",
    "        set_labels(axes, title=title, \n",
    "                   xlabel=xlabel, ylabel=ylabel,\n",
    "                   x_tick_params=dict(labelrotation=90))\n",
    "        fig = axes.get_figure()\n",
    "        fig.set_size_inches(10, df.shape[0]/2)\n",
    "        fig.set_dpi(120)\n",
    "        return fig\n",
    "    \n",
    "def draw_score_heatmap(col, results, type_filter='all', metadata=None, y_sort_by=None,\n",
    "                       filename=None, **kwargs):\n",
    "    df = (results.groupby(['type', 'task', 'framework'])[col]\n",
    "                 .mean()\n",
    "                 .unstack())\n",
    "    df = (df if type_filter == 'all'\n",
    "             else df[df.index.get_loc(type_filter)])\n",
    "    if metadata and y_sort_by:\n",
    "        sort_by = lambda row: row.task.apply(lambda t: getattr(metadata[t], y_sort_by))\n",
    "        df = sort_dataframe(df, by=sort_by)\n",
    "    \n",
    "    fig = draw_heatmap(df, \n",
    "                       y_labels=task_labels(df.index), \n",
    "#                        xlabel=\"Framework\", ylabel=\"Task\",\n",
    "                       **kwargs)\n",
    "    if filename is not None:\n",
    "        savefig(fig, create_file(\"graphics\", results_group, filename))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap_results = res.results.loc[~res.results.framework.isin(['constantpredictor', 'randomforest'])]\n",
    "heatmap_results = res.results.loc[~res.results.framework.isin(['constantpredictor'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_heatmap('imp_score',\n",
    "                   results=heatmap_results,\n",
    "                   type_filter='binary', \n",
    "                   metadata=res.metadata,\n",
    "                   y_sort_by=tasks_sort_by,\n",
    "                   title=f\"Scores ({binary_score_label}) on {results_group} binary classification problems\",\n",
    "                   filename=\"binary_score_heat.png\",\n",
    "                   center=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_heatmap('imp_score', \n",
    "                   results=heatmap_results,\n",
    "                   type_filter='multiclass', \n",
    "                   y_sort_by=tasks_sort_by,\n",
    "                   title=f\"Scores ({multiclass_score_label}) on {results_group} multi-class classification problems\",\n",
    "                   filename=\"multiclass_score_heat.png\",\n",
    "                   center=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_heatmap('norm_score', \n",
    "                   results=heatmap_results,\n",
    "                   type_filter='binary', \n",
    "                   y_sort_by=tasks_sort_by,\n",
    "                   title=f\"Normalized scores on {results_group} binary classification problems\",\n",
    "                   filename=\"binary_norm_score_heat.png\",\n",
    "                   center=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_heatmap('norm_score',\n",
    "                   results=heatmap_results,\n",
    "                   type_filter='multiclass', \n",
    "                   y_sort_by=tasks_sort_by,\n",
    "                   title=f\"Normalized scores on {results_group} multi-class classification problems\",\n",
    "                   filename=\"multiclass_norm_score_heat.png\",\n",
    "                   center=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def draw_parallel_coord(df, class_column, \n",
    "                        x_labels=True, yscale='linear', \n",
    "                        title=None, xlabel=None, ylabel=None,\n",
    "                        legend_loc='best', legend_title=None, colormap=colormap):\n",
    "    with sb.axes_style('ticks', rc={'grid.linestyle': 'dotted'}), sb.plotting_context('paper'):\n",
    "#         print(sb.axes_style())\n",
    "        parallel_fig = mp.pyplot.figure(dpi=120, figsize=(10, df.shape[0]))\n",
    "        # select the first colors from the colormap to ensure we use the same colors as in the stripplot later\n",
    "        colors = mp.cm.get_cmap(colormap).colors[:len(df[class_column].unique())]\n",
    "        axes = pd.plotting.parallel_coordinates(df, \n",
    "                                                class_column=class_column, \n",
    "                                                colors=colors,\n",
    "                                                axvlines=False,\n",
    "                                               )\n",
    "        set_scales(axes, yscale=yscale)\n",
    "        handles, labels = axes.get_legend_handles_labels()\n",
    "        axes.legend(handles, labels, loc=legend_loc, title=legend_title)\n",
    "        set_labels(axes, title=title, xlabel=xlabel, ylabel=ylabel, x_labels=x_labels,\n",
    "                   x_tick_params=dict(labelrotation=90))\n",
    "        return parallel_fig\n",
    "\n",
    "\n",
    "def draw_score_parallel_coord(col, results, type_filter='all', metadata=None,\n",
    "                              x_sort_by='name', ylabel=None, filename=None,\n",
    "                              **kwargs):\n",
    "    res_group = results.groupby(['type', 'task', 'framework'])\n",
    "    df = res_group[col].mean().unstack(['type', 'task'])\n",
    "    df = df if type_filter == 'all' \\\n",
    "            else df.iloc[:, df.columns.get_loc(type_filter)]\n",
    "    if metadata:\n",
    "        sort_by = lambda cols: getattr(metadata[cols[1]], x_sort_by)\n",
    "        df = sort_dataframe(df, by=sort_by, axis=1)\n",
    "    df.reset_index(inplace=True)\n",
    "    fig = draw_parallel_coord(df, \n",
    "                              'framework',\n",
    "                              x_labels=task_labels(df.columns.drop('framework')),\n",
    "#                               xlabel=\"Task\",\n",
    "                              ylabel=ylabel or \"Score\",\n",
    "                              legend_title=\"Framework\",\n",
    "                              **kwargs) \n",
    "    if filename is not None:\n",
    "        savefig(fig, create_file(\"graphics\", results_group, filename))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel_coord_results = res.results.loc[~res.results.framework.isin(['randomforest'])]\n",
    "parallel_coord_results = res.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_parallel_coord('imp_score',\n",
    "                          results=parallel_coord_results,\n",
    "                          type_filter='binary', \n",
    "                          metadata=res.metadata,\n",
    "                          x_sort_by=tasks_sort_by,\n",
    "                          title=f\"Scores ({binary_score_label}) on {results_group} binary classification problems\",\n",
    "                          ylabel=binary_score_label,\n",
    "                          legend_loc='lower left',\n",
    "                          filename=\"binary_score_parallel_ccord.png\"\n",
    "                         );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_parallel_coord('imp_score',\n",
    "                          results=parallel_coord_results,\n",
    "                          type_filter='multiclass',\n",
    "                          metadata=res.metadata,\n",
    "                          x_sort_by=tasks_sort_by,\n",
    "                          title=f\"Scores ({multiclass_score_label}) on {results_group} multi-class classification problems\",\n",
    "                          ylabel=multiclass_score_label,\n",
    "                          yscale=('symlog', dict(linthreshy=0.5)),\n",
    "                          legend_loc='lower left',\n",
    "                          filename=\"multiclass_score_parallel_ccord.png\"\n",
    "                         );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_parallel_coord('norm_score', \n",
    "                          results=parallel_coord_results,\n",
    "                          type_filter='binary', \n",
    "                          metadata=res.metadata,\n",
    "                          x_sort_by=tasks_sort_by,\n",
    "                          title=f\"Normalized scores on {results_group} binary classification problems\",\n",
    "                          filename=\"binary_norm_score_parallel_ccord.png\"\n",
    "                         );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_parallel_coord('norm_score', \n",
    "                          results=parallel_coord_results,\n",
    "                          type_filter='multiclass',\n",
    "                          metadata=res.metadata,\n",
    "                          x_sort_by=tasks_sort_by,\n",
    "                          title=f\"Normalized scores on {results_group} multi-class classification problems\",\n",
    "                          filename=\"multiclass_norm_score_parallel_ccord.png\", \n",
    "                          yscale='symlog',\n",
    "                         );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def draw_stripplot(df, x, y, hue, \n",
    "                   xscale='linear', xbound=None, \n",
    "                   xlabel=None, ylabel=None, y_labels=None, title=None,\n",
    "                   legend_title=None, legend_loc='best', colormap=colormap):\n",
    "    with sb.axes_style('whitegrid', rc={'grid.linestyle': 'dotted'}), sb.plotting_context('paper'):\n",
    "#         print(sb.axes_style())\n",
    "        # Initialize the figure\n",
    "        strip_fig, axes = mp.pyplot.subplots(dpi=120, figsize=(10, len(df.index.unique())))\n",
    "        set_scales(axes, xscale=xscale)\n",
    "        if xbound is not None:   \n",
    "            axes.set_autoscalex_on(False)\n",
    "            axes.set_xbound(*xbound)\n",
    "#             axes.invert_xaxis()\n",
    "        sb.despine(bottom=True, left=True)\n",
    "\n",
    "        # Show each observation with a scatterplot\n",
    "        sb.stripplot(x=x, y=y, hue=hue,\n",
    "                     data=df, dodge=True, jitter=True, palette=colormap,\n",
    "                     alpha=.25, zorder=1)\n",
    "\n",
    "        # Show the conditional means\n",
    "        sb.pointplot(x=x, y=y, hue=hue,\n",
    "                     data=df, dodge=.5, join=False, palette=colormap,\n",
    "                     markers='d', scale=.75, ci=None)\n",
    "\n",
    "        # Improve the legend \n",
    "        handles, labels = axes.get_legend_handles_labels()\n",
    "        dist = int(len(labels)/2)\n",
    "        axes.legend(handles[dist:], labels[dist:], title=legend_title or hue,\n",
    "                    handletextpad=0, columnspacing=1,\n",
    "                    loc=legend_loc, ncol=1, frameon=True)\n",
    "        set_labels(axes, title=title, xlabel=xlabel, ylabel=ylabel, y_labels=y_labels)\n",
    "        return strip_fig\n",
    " \n",
    "\n",
    "def draw_score_stripplot(col, results, type_filter='all', metadata=None,\n",
    "                         y_sort_by='name', filename=None, **kwargs):\n",
    "    sort_by = (None if not metadata \n",
    "               else lambda row: row.task.apply(lambda t: getattr(metadata[t], y_sort_by)))\n",
    "    scatterplot_df = sort_dataframe(results.set_index(['type', 'task']), by=sort_by)\n",
    "    df = scatterplot_df if type_filter == 'all' \\\n",
    "                        else scatterplot_df[scatterplot_df.index.get_loc(type_filter)]\n",
    "    fig = draw_stripplot(\n",
    "        df,\n",
    "        x=col,\n",
    "        y=df.index,\n",
    "        hue='framework',\n",
    "#         ylabel='Task',\n",
    "        y_labels=task_labels(df.index.unique()),\n",
    "        legend_title=\"Framework\",\n",
    "        **kwargs\n",
    "    )\n",
    "    if filename is not None:\n",
    "        savefig(fig, create_file(\"graphics\", results_group, filename))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot_results = (res.results.loc[~res.results.framework.isin(['randomforest'])]\n",
    "#                                   .sort_values(by=['framework']))  # sorting for colors consistency\n",
    "scatterplot_results = res.results.sort_values(by=['framework'])  # sorting for colors consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_stripplot('imp_result', \n",
    "                     results=scatterplot_results,\n",
    "                     type_filter='binary', \n",
    "                     metadata=res.metadata,\n",
    "                     y_sort_by=tasks_sort_by,\n",
    "                     title=f\"Scores ({binary_score_label}) on {results_group} binary classification problems\",\n",
    "                     xlabel=binary_score_label,\n",
    "                     filename=\"binary_results_stripplot.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_stripplot('imp_result',\n",
    "                     results=scatterplot_results,\n",
    "                     type_filter='multiclass', \n",
    "                     metadata=res.metadata,\n",
    "                     y_sort_by=tasks_sort_by,\n",
    "#                      xbound=(0,10),\n",
    "                     xscale=('symlog', dict(linthreshx=0.5)),\n",
    "                     title=f\"Scores ({multiclass_score_label}) on {results_group} multi-class classification problems\",\n",
    "                     xlabel=multiclass_score_label, \n",
    "                     filename=\"multiclass_results_stripplot.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_stripplot('norm_score', \n",
    "                     results=scatterplot_results,\n",
    "                     type_filter='binary', \n",
    "                     metadata=res.metadata,\n",
    "                     y_sort_by=tasks_sort_by,\n",
    "                     xbound=(-0.2, 2),\n",
    "                     xscale='linear',\n",
    "                     title=f\"Normalized scores on {results_group} binary classification problems\",\n",
    "                     filename=\"binary_norm_score_stripplot.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_stripplot('norm_score', \n",
    "                     results=scatterplot_results,\n",
    "                     type_filter='multiclass', \n",
    "                     metadata=res.metadata,\n",
    "                     y_sort_by=tasks_sort_by,\n",
    "                     xbound=(-0.2, 2.5),\n",
    "                     xscale='linear',\n",
    "                     title=f\"Normalized scores on {results_group} multi-class classification problems\",\n",
    "                     filename=\"multiclass_norm_score_stripplot.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Framework Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res.results.loc[(res.results.task.str.contains('jungle'))&(res.results.framework=='tunedrandomforest')];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "done.iloc[done.index.get_level_values('framework').isin(['autosklearn', 'h2oautoml', 'tpot'])]\\\n",
    "    .apply(sorted_ints);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "failures = res.failed.groupby(['task', 'fold', 'framework'])['info']\\\n",
    "                     .unique()\n",
    "#display(failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h2o_res = (res.results.loc[(res.results.framework=='h2oautoml')&(res.results.fold==0)]\n",
    "                     [['task', 'result', 'acc']])\n",
    "# display(tabulate(h2o_res, h2o_res.columns, tablefmt='grid'))\n",
    "display(h2o_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve37-h2o",
   "language": "python",
   "name": "ve37-h2o"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
